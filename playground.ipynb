{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "589e16cc31d7401ab943b894769ae3c0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "95880210fc7842e19a3c6c0fa352f78a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 106,
    "execution_start": 1668534607328,
    "source_hash": "c057d76e"
   },
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus, relu\n",
    "from torch.distributions import Distribution, Normal\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from gmfpp.utils.data_preparation import *\n",
    "from gmfpp.utils.data_transformers import *\n",
    "from gmfpp.utils.plotting import *\n",
    "\n",
    "from gmfpp.models.ReparameterizedDiagonalGaussian import *\n",
    "from gmfpp.models.CytoVariationalAutoencoder import *\n",
    "from gmfpp.models.VariationalAutoencoder import *\n",
    "from gmfpp.models.ConvVariationalAutoencoder import *\n",
    "from gmfpp.models.VariationalInference import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_seed(seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ccf34ec649449f9897718de613e77a3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "7f138156e99b419da71decf6b0ddd537",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 70,
    "execution_start": 1668533823880,
    "source_hash": "f50043b7"
   },
   "outputs": [],
   "source": [
    "metadata_all = read_metadata(\"./data/all/metadata.csv\")\n",
    "mapping = get_MOA_mappings(metadata_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = read_metadata(\"./data/tiny/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = shuffle_metadata(metadata)\n",
    "metadata_train_all, metadata_test = split_metadata(metadata, split_fraction = .90)\n",
    "metadata_train, metadata_validation = split_metadata(metadata_train_all, split_fraction = .90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "0a35af5f97ba47e2a7a51cb5919fc704",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1668533825380,
    "source_hash": "268f0f7"
   },
   "outputs": [],
   "source": [
    "relative_path = get_relative_image_paths(metadata)\n",
    "image_paths = [\"./data/tiny/\" + path for path in relative_path]\n",
    "images = load_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "4009d3104a144cc0839232fdad83b6fb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 49,
    "execution_start": 1668533825828,
    "source_hash": "7b6291e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([259, 3, 68, 68])\n"
     ]
    }
   ],
   "source": [
    "images = prepare_raw_images(images)\n",
    "normalize_channels_inplace(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 0 interval: [0.02; 1.00]\n",
      "channel 1 interval: [0.04; 1.00]\n",
      "channel 2 interval: [0.05; 1.00]\n"
     ]
    }
   ],
   "source": [
    "channel_first = view_channel_dim_first(images)\n",
    "for i in range(channel_first.shape[0]):\n",
    "    channel = channel_first[i]\n",
    "    print(\"channel {} interval: [{:.2f}; {:.2f}]\".format(i, torch.min(channel), torch.max(channel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame, images: torch.Tensor, label_to_id: Dict[str, int]):\n",
    "        self.metadata = metadata\n",
    "        self.label_to_id = label_to_id\n",
    "        self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        \n",
    "        label_name = row[\"moa\"]\n",
    "        label = self.label_to_id[label_name]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SingleCellDataset(metadata_train, images, mapping)\n",
    "validation_set = SingleCellDataset(metadata_validation, images, mapping)\n",
    "test_set = SingleCellDataset(metadata_test, images, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "768e10612c1d4ecd85db88848139700a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "44175fa2e3b44a76bc94b5082c485977",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 127,
    "execution_start": 1668533845832,
    "source_hash": "66b0d064"
   },
   "outputs": [],
   "source": [
    "image_shape = np.array([3, 68, 68])\n",
    "latent_features = 256\n",
    "\n",
    "vae = CytoVariationalAutoencoder(image_shape, latent_features)\n",
    "#vae = VariationalAutoencoder(image_shape, latent_features)\n",
    "vae = vae.to(device)\n",
    "\n",
    "beta = 1.\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 10e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "bd5c4dfc160b4febbdaf2209398fde60",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15978,
    "execution_start": 1668533848529,
    "source_hash": "8bd36839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/10\n",
      "training | elbo: -14701.419271, log_px: -13283.28, kl: 1418.14:\n",
      "validation | elbo: -13014.904297, log_px: -12988.78, kl: 26.12:\n",
      "epoch: 1/10\n",
      "training | elbo: -13492.100098, log_px: -12836.09, kl: 656.01:\n",
      "validation | elbo: -13081.771973, log_px: -12945.89, kl: 135.89:\n",
      "epoch: 2/10\n",
      "training | elbo: -13293.366699, log_px: -12798.09, kl: 495.27:\n",
      "validation | elbo: -13272.919271, log_px: -12909.20, kl: 363.72:\n",
      "epoch: 3/10\n",
      "training | elbo: -13179.118978, log_px: -12775.27, kl: 403.85:\n",
      "validation | elbo: -13558.588379, log_px: -12877.05, kl: 681.54:\n",
      "epoch: 4/10\n",
      "training | elbo: -13127.640951, log_px: -12765.65, kl: 361.99:\n",
      "validation | elbo: -13673.250195, log_px: -12860.11, kl: 813.14:\n",
      "epoch: 5/10\n",
      "training | elbo: -13095.499512, log_px: -12756.63, kl: 338.87:\n",
      "validation | elbo: -13699.078776, log_px: -12843.98, kl: 855.10:\n",
      "epoch: 6/10\n",
      "training | elbo: -13059.161458, log_px: -12750.91, kl: 308.25:\n",
      "validation | elbo: -13660.979074, log_px: -12832.04, kl: 828.94:\n",
      "epoch: 7/10\n",
      "training | elbo: -13033.097819, log_px: -12743.30, kl: 289.79:\n",
      "validation | elbo: -13602.128662, log_px: -12821.14, kl: 780.99:\n",
      "epoch: 8/10\n",
      "training | elbo: -13003.924642, log_px: -12735.97, kl: 267.96:\n",
      "validation | elbo: -13543.688151, log_px: -12812.16, kl: 731.53:\n",
      "epoch: 9/10\n",
      "training | elbo: -12984.829102, log_px: -12729.38, kl: 255.45:\n",
      "validation | elbo: -13486.179004, log_px: -12803.24, kl: 682.94:\n"
     ]
    }
   ],
   "source": [
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch: {epoch}/{num_epochs}\")    \n",
    "\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(vae.parameters(), 10_000)\n",
    "        optimizer.step()\n",
    "\n",
    "        # gather data for the current batch\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "\n",
    "    print(\"training | elbo: {:2f}, log_px: {:.2f}, kl: {:.2f}:\".format(np.mean(training_epoch_data[\"elbo\"]), np.mean(training_epoch_data[\"log_px\"]), np.mean(training_epoch_data[\"kl\"])))\n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "\n",
    "        # Just load a single batch from the test loader\n",
    "        '''x, y = next(iter(test_loader))'''\n",
    "        x = x.to(device)\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "\n",
    "    print(\"validation | elbo: {:2f}, log_px: {:.2f}, kl: {:.2f}:\".format(np.mean(validation_data[\"elbo\"]), np.mean(validation_data[\"log_px\"]), np.mean(validation_data[\"kl\"])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d741e8e33b04435db434287332c9e96a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 784,
    "execution_start": 1668533868552,
    "source_hash": "88e844e"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_data[\"elbo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38f55a130f7542d5bcbb1afb22d361bf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Compare reconstruction and original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "845aa77120a94f608bcbf4ae933e7609",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1668533879605,
    "source_hash": "9aa1ea6c"
   },
   "outputs": [],
   "source": [
    "x = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cfad113524184a279df7d73b4906be9f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 717,
    "execution_start": 1668533891834,
    "source_hash": "f71a79cc"
   },
   "outputs": [],
   "source": [
    "plot_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2f0aad8f7c694742a7c4f86541691efa",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1318,
    "execution_start": 1668533896933,
    "source_hash": "59410032",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae.eval() # because of batch normalization\n",
    "outputs = vae(x[None,:,:,:])\n",
    "px = outputs[\"px\"]\n",
    "\n",
    "x_reconstruction = px.sample()\n",
    "x_reconstruction = x_reconstruction[0]\n",
    "plot_image_channels(x_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a38720c94a247c38a7836f9bf2f80e3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1513,
    "execution_start": 1668533900217,
    "source_hash": "b4c9cf8a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_reconstruction = px.sample()\n",
    "x_reconstruction = x_reconstruction[0]\n",
    "plot_image(clip_image_to_zero_one(x_reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "19a830d34463493c9f716e5189fead55",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1668531557330,
    "source_hash": "b623e53d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @TODO cleanup. Used to images from cluster quickly\n",
    "#x_reconstruction = torch.tensor(np.array(load_images([\"./dump/images/x0_reconstruction.npy\"])[0], dtype=np.float32))\n",
    "#x = torch.tensor(np.array(load_images([\"./dump/images/x0.npy\"])[0], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_channels(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_channels(x_reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=13, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes: int = 13):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "N_classes = len(mapping)\n",
    "classifier = NeuralNetwork(N_classes).to(device)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame, images: torch.Tensor, label_to_id: Dict[str, int]):\n",
    "        self.metadata = metadata\n",
    "        self.label_to_id = label_to_id\n",
    "        self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        \n",
    "        label_name = row[\"moa\"]\n",
    "        label = self.label_to_id[label_name]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE\n",
    "image_shape = np.array([3, 68, 68])\n",
    "latent_features = 256\n",
    "vae = CytoVariationalAutoencoder(image_shape, latent_features) # @TODO: load trained parameters\n",
    "vae.eval()\n",
    "\n",
    "# Classifier\n",
    "N_classes = len(mapping)\n",
    "classifier = NeuralNetwork(N_classes).to(device)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/5\n",
      "training | loss: 2.80\n",
      "epoch: 1/5\n",
      "training | loss: 2.61\n",
      "epoch: 2/5\n",
      "training | loss: 2.43\n",
      "epoch: 3/5\n",
      "training | loss: 2.24\n",
      "epoch: 4/5\n",
      "training | loss: 2.07\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch: {epoch}/{num_epochs}\")    \n",
    "\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        \n",
    "        outputs = vae(x)\n",
    "        z = outputs[\"z\"]\n",
    "        \n",
    "        pred = classifier(z)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        train_epoch_loss.append(loss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    print(\"training | loss: {:.2f}\".format(np.mean(train_epoch_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @TODO\n",
    "- Look at latent representation\n",
    "    - How does changing one latent variable change the image reconstruction?\n",
    "    - How similiar are images in the latent space (cosine-simularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d2059ce0701f4cb4bc01ee931dc0f39b",
  "deepnote_persisted_session": {
   "createdAt": "2022-11-14T22:47:34.652Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
